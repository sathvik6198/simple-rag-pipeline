# ğŸ§  Simple RAG Pipeline (Fully Local with Ollama + LLaMA3)

This project is a beginner-friendly, end-to-end implementation of a **Retrieval-Augmented Generation (RAG)** pipeline, running fully **locally** using [Ollama](https://ollama.com/) and [LLaMA3](https://ollama.com/library/llama3). It demonstrates how to ingest documents, embed and store them, search for relevant content, generate answers, and evaluate accuracyâ€”all through a clean and modular CLI.

![RAG Image](./rag-design-basic.png)

---

## ğŸš€ Features

- âœ… Fully Local LLM with **Ollama + LLaMA3**
- ğŸ“„ Index PDFs or plain text documents using custom or Docling-based indexers
- ğŸ” Embed text using **nomic-embed-text** (also via Ollama)
- ğŸ§  Vector search using **LanceDB**
- ğŸ§µ Modular components for indexing, retrieval, generation, and evaluation
- ğŸ“Š CLI commands for adding, querying, and evaluating

---

## ğŸ§± Architecture

The pipeline is modular and each part is responsible for a specific task:

| File/Module             | Responsibility                              |
|-------------------------|----------------------------------------------|
| `datastore.py`          | Vector storage and retrieval using LanceDB   |
| `indexer.py`            | Parsing and chunking documents               |
| `retriever.py`          | Finding relevant chunks using similarity     |
| `response_generator.py` | LLM-based answer generation with LLaMA3      |
| `evaluator.py`          | Comparing generated answers vs ground truth  |
| `rag_pipeline.py`       | Orchestrates the full pipeline               |

Each component has a base class (`interface/`) so it can be swapped or extended easily.

---

## âš™ï¸ Installation

### 1. Clone the Repo

```bash
git clone https://github.com/sathvik6198/simple-rag-pipeline.git
cd simple-rag-pipeline
```

### 2. Set Up Environment (Recommended)

```bash
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows
```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

> âš ï¸ Ensure `numpy` version is `<2.0` due to compatibility with LanceDB.

---

## ğŸ”§ Ollama Setup

Install Ollama from: https://ollama.com/download  
Then pull the required models:

```bash
ollama pull llama3
ollama pull nomic-embed-text
```

---

## ğŸ“ Folder Structure

```bash
.
â”œâ”€â”€ main.py                  # CLI entry point
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ rag_pipeline.py      # Main RAG orchestrator
â”‚   â”œâ”€â”€ impl/                # Implementation of pipeline steps
â”‚   â”œâ”€â”€ util/                # Ollama client, embedding utilities
â”‚   â””â”€â”€ interface/           # Base interfaces for each component
â”œâ”€â”€ sample_data/
â”‚   â”œâ”€â”€ source/              # Add your PDFs or text here
â”‚   â””â”€â”€ eval/                # Evaluation question/answer pairs
```

---

## ğŸ§ª CLI Usage

### Reset Vector Store

```bash
python main.py reset
```

### Index Documents

```bash
python main.py add -p "sample_data/source/"
```

### Query the System

```bash
python main.py query "What is the opening year of The Lagoon Breeze Hotel?"
```

### Evaluate with JSON File

```bash
python main.py evaluate -f "sample_data/eval/sample_questions.json"
```

---

## âœ… Requirements

```txt
pydantic>=2.0.0
lancedb==0.22.0
docling==2.31.0
cohere==5.15.0  # Optional â€“ used for reranking (can be removed)
```

---
---

## ğŸ³ Docker Usage

You can run the entire RAG pipeline inside a Docker container without installing Python or dependencies locally.

### ğŸ”¨ Build the Docker Image

```bash
docker build -t rag-pipeline .
```

### ğŸš€ Run the Pipeline

Start the container and run the default command:

```bash
docker run --rm -it \
  -v $(pwd)/sample_data:/app/sample_data \
  -p 11434:11434 \
  rag-pipeline
```

> ğŸ’¡ Make sure [Ollama](https://ollama.com/) is running on your host machine (`ollama serve`) before starting the container.

### â“ Example: Query the Pipeline

```bash
docker run --rm -it \
  -v $(pwd)/sample_data:/app/sample_data \
  rag-pipeline python main.py query "What is the Lagoon Breeze Hotel?"
```

### ğŸ›‘ Clean Up

Since weâ€™re using `--rm`, the container will delete itself after exiting. No cleanup needed!

## ğŸ™ Acknowledgements

This project is inspired by [**pixegami/simple-rag-pipeline**], which provides a clean and extensible foundation for RAG.

My version includes:

- ğŸ”’ **Full local inference** using **Ollama + LLaMA3**
- ğŸ§  Swapped embedding model to **nomic-embed-text** via Ollama
- ğŸ§¹ Removed OpenAI dependency
- âš™ï¸ Modular enhancements for easier local experimentation
