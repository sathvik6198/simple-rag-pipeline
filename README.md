# ğŸ§  Simple RAG Pipeline (Fully Local with Ollama + LLaMA3)

This project is a beginner-friendly, end-to-end implementation of a **Retrieval-Augmented Generation (RAG)** pipeline, running fully **locally** using [Ollama](https://ollama.com/) and [LLaMA3](https://ollama.com/library/llama3). It demonstrates how to ingest documents, embed and store them, search for relevant content, generate answers, and evaluate accuracyâ€”all through a clean and modular CLI.

![RAG Image](./rag-design-basic.png)

---

## ğŸš€ Features

- âœ… Fully Local LLM with **Ollama + LLaMA3**
- ğŸ“„ Index PDFs or plain text documents using custom or Docling-based indexers
- ğŸ” Embed text using **nomic-embed-text** (also via Ollama)
- ğŸ§  Vector search using **LanceDB**
- ğŸ§µ Modular components for indexing, retrieval, generation, and evaluation
- ğŸ“Š CLI commands for adding, querying, and evaluating

---

## ğŸ§± Architecture

The pipeline is coordinated by `src/rag_pipeline.py` and includes:

- **Datastore:** Stores and retrieves vector embeddings via **LanceDB**
- **Indexer:** Splits and embeds documents using **nomic-embed-text** via Ollama
- **Retriever:** Performs similarity search and optional reranking
- **Response Generator:** Uses **LLaMA3** locally through Ollama
- **Evaluator:** Compares generated answers to expected ones from eval sets

Each component has a base class (`interface/`) so it can be swapped or extended easily.

---

## âš™ï¸ Installation

### 1. Clone the Repo

```bash
git clone https://github.com/sathvik6198/simple-rag-pipeline.git
cd simple-rag-pipeline
```

### 2. Set Up Environment (Recommended)

```bash
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows
```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

> âš ï¸ Ensure `numpy` version is `<2.0` due to compatibility with LanceDB.

---

## ğŸ”§ Ollama Setup

Install Ollama from: https://ollama.com/download  
Then pull the required models:

```bash
ollama pull llama3
ollama pull nomic-embed-text
```

---

## ğŸ“ Folder Structure

```bash
.
â”œâ”€â”€ main.py                  # CLI entry point
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ rag_pipeline.py      # Main RAG orchestrator
â”‚   â”œâ”€â”€ impl/                # Implementation of pipeline steps
â”‚   â”œâ”€â”€ util/                # Ollama client, embedding utilities
â”‚   â””â”€â”€ interface/           # Base interfaces for each component
â”œâ”€â”€ sample_data/
â”‚   â”œâ”€â”€ source/              # Add your PDFs or text here
â”‚   â””â”€â”€ eval/                # Evaluation question/answer pairs
```

---

## ğŸ§ª CLI Usage

### Reset Vector Store

```bash
python main.py reset
```

### Index Documents

```bash
python main.py add -p "sample_data/source/"
```

### Query the System

```bash
python main.py query "What is the opening year of The Lagoon Breeze Hotel?"
```

### Evaluate with JSON File

```bash
python main.py evaluate -f "sample_data/eval/sample_questions.json"
```

---

## âœ… Requirements

```txt
pydantic>=2.0.0
lancedb==0.22.0
docling==2.31.0
cohere==5.15.0  # Optional â€“ used for reranking (can be removed)
```

---

## ğŸ™ Acknowledgements

This project is inspired by [**pixegami/simple-rag-pipeline**](https://github.com/pixegami/simple-rag-pipeline), which provides a clean and extensible foundation for RAG.

My version includes:

- ğŸ”’ **Full local inference** using **Ollama + LLaMA3**
- ğŸ§  Swapped embedding model to **nomic-embed-text** via Ollama
- ğŸ§¹ Removed OpenAI dependency
- âš™ï¸ Modular enhancements for easier local experimentation
